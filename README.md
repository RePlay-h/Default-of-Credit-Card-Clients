# Default of Credit Card Clients

This project focuses on predicting the probability of default for credit card holders based on customer data from Taiwanese banks.  
The dataset is taken from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients).  

Predicting defaults is important for banks as it helps reduce credit risk, adjust credit limits, and optimize lending decisions.  

---

## ğŸ“Š Dataset

The raw data contains information on **30,000 clients**, including:  
- Credit limit (`LIMIT_BAL`)  
- Gender, education, marital status, age  
- Payment history over 6 months (`PAY_0 â€¦ PAY_6`)  
- Bill amounts (`BILL_AMT1 â€¦ BILL_AMT6`)  
- Payment amounts (`PAY_AMT1 â€¦ PAY_AMT6`)  
- Target variable `default_payment_next_month` â€” whether the client defaulted in the next month  

The classes are highly imbalanced (about 22% defaults), so handling class imbalance was taken into account during model training.  

---

## ğŸ› ï¸ Feature Engineering

To improve model performance, new features were added:

1. **Number of Delinquencies**  
   Sum of indicators (PAY_i > 0) across all months  

2. **Debt Ratio**  
   Ratio of total debt to credit limit  

3. **Payment Ratio**  
   Ratio of debt to actual payment made  

---

## ğŸ¤– Models and Metrics

- **Logistic Regression** â€” baseline model  
- **CatBoostClassifier** â€” main model, hyperparameters tuned with Optuna (TPE algorithm)  

The main metric is **AUC ROC**, due to class imbalance. Additionally, precision, recall, and F1-score were measured.  

### Logistic Regression
- AUC: `0.6396`  
- Precision: `0.0`  
- Recall: `0.0`  
- F1: `0.0`  

Confusion matrix:
```
[[7056 2]
[1942 0]]
```

### CatBoost
- AUC: `0.7801`  
- Precision: `0.6877`  
- Recall: `0.3584`  
- F1: `0.4712`  

Confusion matrix:
```
[[6742 316]
[1246 696]]
```

ğŸ“Œ CatBoost significantly outperformed Logistic Regression and showed a more balanced precision-recall performance.  

---

## ğŸ“‚ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
```
â”œâ”€â”€ .dvc/ # ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ DVC
â”œâ”€â”€ catboost_info/ # ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ CatBoost
â”œâ”€â”€ data/ # Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
â”‚ â”œâ”€â”€ raw/ # Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
â”‚ â”‚ â””â”€â”€ clients.xls
â”‚ â”œâ”€â”€ processed/ # Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
â”‚ â”‚ â””â”€â”€ data.csv
â”‚ â””â”€â”€ test/ # Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
â”‚ â”œâ”€â”€ X_test.csv
â”‚ â””â”€â”€ y_test.csv
â”œâ”€â”€ env/ # Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ
â”œâ”€â”€ metrics/ # Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
â”‚ â””â”€â”€ metrics.json
â”œâ”€â”€ model/ # ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
â”‚ â”œâ”€â”€ baseline.pkl # Logistic Regression
â”‚ â””â”€â”€ boosting.pkl # CatBoost
â”œâ”€â”€ src/ # Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´
â”‚ â”œâ”€â”€ preprocess.py # Ğ¿Ñ€ĞµĞ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¸Ğ½Ğ³ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â”‚ â”œâ”€â”€ train.py # Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
â”‚ â””â”€â”€ test.py # Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
â”œâ”€â”€ EDA.ipynb # Ñ€Ğ°Ğ·Ğ²ĞµĞ´Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â”œâ”€â”€ params.yaml # Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
â”œâ”€â”€ dvc.yaml # Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ DVC
â”œâ”€â”€ dvc.lock # lock-Ñ„Ğ°Ğ¹Ğ» DVC
â”œâ”€â”€ requirements.txt # Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
â””â”€â”€ README.md # Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
```
## âœ… Conclusions

- Logistic Regression struggled with class imbalance and almost never predicted the positive class  
- CatBoost achieved a significant improvement (AUC â‰ˆ 0.78) and better balanced precision and recall  
- Future improvements could include:  
  - using **PR AUC** as the main metric  
  - handling class imbalance with methods like SMOTE, undersampling, or scale_pos_weight
  